{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制（Attention Mechanism）与其中的查询、键和值\n",
    "\n",
    "注意力机制（Attention Mechanism）是一种在深度学习模型中广泛使用的机制，特别是在自然语言处理（NLP）任务中，如机器翻译、文本摘要、问答系统等。注意力机制允许模型在处理输入序列时，动态地关注输入序列的不同部分，从而提高模型的性能。\n",
    "\n",
    "## 1. 基本概念\n",
    "\n",
    "### 1.1 背景\n",
    "\n",
    "在传统的序列到序列（Seq2Seq）模型中，编码器将整个输入序列压缩成一个固定大小的上下文向量（Context Vector），然后解码器根据这个向量生成输出序列。然而，这种压缩可能导致信息的丢失，尤其是在处理长序列时。\n",
    "\n",
    "### 1.2 注意力机制\n",
    "\n",
    "注意力机制通过在解码器的每一步生成过程中，动态地计算输入序列中每个元素的重要性（注意力权重），并根据这些权重对输入序列进行加权求和，得到一个加权上下文向量。这个加权上下文向量被用来生成当前的输出元素。\n",
    "\n",
    "## 2. 查询、键和值（Query, Key, Value）\n",
    "\n",
    "在注意力机制中，查询（Query）、键（Key）和值（Value）是三个核心概念。它们分别对应不同的向量，用于计算注意力权重和加权上下文向量。\n",
    "\n",
    "### 2.1 查询（Query）\n",
    "\n",
    "- **查询（Query）**：通常表示解码器当前的隐藏状态。查询用于与编码器的隐藏状态（键）进行比较，计算注意力分数。\n",
    "\n",
    "### 2.2 键（Key）\n",
    "\n",
    "- **键（Key）**：通常表示编码器的隐藏状态。键用于与查询进行比较，计算注意力分数。\n",
    "\n",
    "### 2.3 值（Value）\n",
    "\n",
    "- **值（Value）**：通常也表示编码器的隐藏状态。值用于根据注意力权重进行加权求和，得到加权上下文向量。\n",
    "\n",
    "## 3. 工作原理\n",
    "\n",
    "### 3.1 编码器\n",
    "\n",
    "编码器将输入序列转换为一系列隐藏状态（Hidden States），每个隐藏状态对应输入序列中的一个元素。这些隐藏状态既作为键，也作为值。\n",
    "\n",
    "### 3.2 解码器\n",
    "\n",
    "在解码器的每一步生成过程中，注意力机制执行以下步骤：\n",
    "\n",
    "1. **计算注意力分数**：解码器当前的隐藏状态（查询）与编码器的每个隐藏状态（键）进行比较，计算它们之间的相似度（如点积、余弦相似度等），得到一个注意力分数。\n",
    "2. **归一化**：对注意力分数进行归一化处理（如使用Softmax函数），得到注意力权重。\n",
    "3. **加权求和**：根据注意力权重对编码器的隐藏状态（值）进行加权求和，得到一个加权上下文向量。\n",
    "4. **生成输出**：将加权上下文向量与解码器的当前隐藏状态结合，生成当前的输出元素。\n",
    "\n",
    "![QKV](https://zh-v2.d2l.ai/_images/qkv.svg \"QKV\")\n",
    "\n",
    "## 4. 注意力机制的类型\n",
    "\n",
    "### 4.1 全局注意力（Global Attention）\n",
    "\n",
    "在每一步生成过程中，考虑输入序列的所有隐藏状态（键和值）。\n",
    "\n",
    "### 4.2 局部注意力（Local Attention）\n",
    "\n",
    "在每一步生成过程中，只考虑输入序列的一部分隐藏状态（键和值），通常是与当前生成位置相关的部分。\n",
    "\n",
    "## 5. 优点与局限性\n",
    "\n",
    "### 5.1 优点\n",
    "\n",
    "- **动态关注**：注意力机制允许模型在每一步生成过程中动态地关注输入序列的不同部分，从而更好地捕捉输入序列中的重要信息。\n",
    "- **处理长序列**：相比于传统的Seq2Seq模型，注意力机制在处理长序列时表现更好，因为它不需要将整个输入序列压缩成一个固定大小的向量。\n",
    "\n",
    "### 5.2 局限性\n",
    "\n",
    "- **计算复杂度**：注意力机制的计算复杂度较高，尤其是在处理长序列时。\n",
    "- **可解释性**：虽然注意力机制提高了模型的性能，但它也使得模型的可解释性降低，因为注意力权重是动态计算的，难以直观理解。\n",
    "\n",
    "## 6. 应用场景\n",
    "\n",
    "- **机器翻译**：在生成翻译后的句子时，注意力机制可以帮助模型关注源语言句子中的重要部分。\n",
    "- **文本摘要**：在生成摘要时，注意力机制可以帮助模型关注原文中的关键信息。\n",
    "- **问答系统**：在生成回答时，注意力机制可以帮助模型关注问题中的关键部分和相关上下文。\n",
    "\n",
    "## 7. 改进与变体\n",
    "\n",
    "### 7.1 多头注意力（Multi-Head Attention）\n",
    "\n",
    "多头注意力机制是Transformer模型的核心组件之一。它通过并行计算多个注意力头（Attention Heads），每个头关注输入序列的不同部分，从而捕捉更丰富的信息。\n",
    "\n",
    "### 7.2 自注意力（Self-Attention）\n",
    "\n",
    "自注意力机制允许模型在处理序列时，关注序列中的其他部分。例如，在Transformer模型中，自注意力机制被用来捕捉序列内部的依赖关系。\n",
    "\n",
    "## 8. 总结\n",
    "\n",
    "注意力机制是一种在深度学习模型中广泛使用的机制，特别是在处理序列数据时。通过动态地关注输入序列的不同部分，注意力机制显著提高了模型的性能，尤其是在处理长序列和复杂任务时。尽管计算复杂度较高，但注意力机制在许多NLP任务中表现出色，成为现代深度学习模型的核心组件之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单代码实现\n",
    "\n",
    "### 非参数注意力汇聚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果一个键Xi越是接近给定的查询X， 那么分配给这个键对应值yi的注意力权重就会越大， 也就“获得了更多的注意力”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# X_repeat的形状:(n_test,n_train),\n",
    "# 每一行都包含着相同的测试输入（例如：同样的查询）\n",
    "X_repeat = x_test.repeat(n_train).reshape((-1, n_train))\n",
    "# x_train包含着键。attention_weights的形状：(n_test,n_train),\n",
    "# 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重\n",
    "attention_weights = npx.softmax(-(X_repeat - x_train)**2 / 2)\n",
    "# y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重\n",
    "y_hat = np.dot(attention_weights, y_train)\n",
    "plot_kernel_reg(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带参数注意力汇聚\n",
    "\n",
    "在下面的查询X和键Xi之间的距离乘以可学习参数w：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}\\begin{aligned}f(x) &= \\sum_{i=1}^n \\alpha(x, x_i) y_i \\\\&= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}((x - x_j)w)^2\\right)} y_i \\\\&= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}((x - x_i)w)^2\\right) y_i.\\end{aligned}\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class NWKernelRegression(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = self.params.get('w', shape=(1,))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # queries和attention_weights的形状为(查询数，“键－值”对数)\n",
    "        queries = queries.repeat(keys.shape[1]).reshape((-1, keys.shape[1]))\n",
    "        self.attention_weights = npx.softmax(\n",
    "            -((queries - keys) * self.w.data())**2 / 2)\n",
    "        # values的形状为(查询数，“键－值”对数)\n",
    "        return npx.batch_dot(np.expand_dims(self.attention_weights, 1),\n",
    "                             np.expand_dims(values, -1)).reshape(-1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
