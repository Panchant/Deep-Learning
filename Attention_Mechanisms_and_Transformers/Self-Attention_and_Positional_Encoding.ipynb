{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自注意力（Self-Attention）\n",
    "\n",
    "自注意力（Self-Attention）是一种在深度学习模型中广泛使用的机制，特别是在自然语言处理（NLP）任务中，如机器翻译、文本摘要、问答系统等。自注意力机制允许模型在处理序列数据时，动态地关注序列中的其他部分，从而捕捉序列内部的依赖关系。\n",
    "\n",
    "## 1. 背景\n",
    "\n",
    "在传统的序列到序列（Seq2Seq）模型中，编码器将整个输入序列压缩成一个固定大小的上下文向量（Context Vector），然后解码器根据这个向量生成输出序列。然而，这种压缩可能导致信息的丢失，尤其是在处理长序列时。\n",
    "\n",
    "## 2. 自注意力的核心思想\n",
    "\n",
    "自注意力的核心思想是通过计算序列中每个元素与其他元素之间的相似度，来动态地关注序列的不同部分。具体来说，自注意力机制使用查询（Query）、键（Key）和值（Value）来计算注意力分数，并根据这些分数对序列进行加权求和，得到加权上下文向量。\n",
    "\n",
    "## 3. 工作原理\n",
    "\n",
    "### 3.1 查询、键和值（Query, Key, Value）\n",
    "\n",
    "在自注意力机制中，查询（Query）、键（Key）和值（Value）是三个核心概念。它们分别对应不同的向量，用于计算注意力权重和加权上下文向量。\n",
    "\n",
    "- **查询（Query）**：通常表示序列中的某个元素。查询用于与序列中的其他元素（键）进行比较，计算注意力分数。\n",
    "- **键（Key）**：通常也表示序列中的某个元素。键用于与查询进行比较，计算注意力分数。\n",
    "- **值（Value）**：通常也表示序列中的某个元素。值用于根据注意力权重进行加权求和，得到加权上下文向量。\n",
    "\n",
    "### 3.2 计算注意力分数\n",
    "\n",
    "自注意力机制通过计算查询和键之间的相似度来得到注意力分数。常见的方法包括点积（Dot Product）、缩放点积（Scaled Dot Product）等。\n",
    "\n",
    "- **公式**：\n",
    "  \\[\n",
    "  \\text{score}(Q, K) = \\frac{Q \\cdot K}{\\sqrt{d_k}}\n",
    "  \\]\n",
    "  其中，\\( Q \\) 是查询向量，\\( K \\) 是键向量，\\( d_k \\) 是键向量的维度。\n",
    "\n",
    "### 3.3 归一化\n",
    "\n",
    "计算得到注意力分数后，通常需要对其进行归一化处理，以得到注意力权重。归一化方法通常使用 Softmax 函数，将注意力分数转换为概率分布。\n",
    "\n",
    "- **公式**：\n",
    "  \\[\n",
    "  \\alpha_i = \\text{softmax}(\\text{score}(Q, K_i))\n",
    "  \\]\n",
    "  其中，\\( \\alpha_i \\) 是第 \\( i \\) 个输入元素的注意力权重。\n",
    "\n",
    "### 3.4 加权求和\n",
    "\n",
    "根据归一化后的注意力权重，对序列中的元素（值）进行加权求和，得到加权上下文向量。\n",
    "\n",
    "- **公式**：\n",
    "  \\[\n",
    "  \\text{context} = \\sum_i \\alpha_i \\cdot V_i\n",
    "  \\]\n",
    "  其中，\\( V_i \\) 是第 \\( i \\) 个输入元素的值向量。\n",
    "\n",
    "![SA](https://zh-v2.d2l.ai/_images/cnn-rnn-self-attention.svg \"SA\")\n",
    "\n",
    "## 4. 优点与局限性\n",
    "\n",
    "### 4.1 优点\n",
    "\n",
    "- **动态关注**：自注意力机制允许模型在每一步生成过程中动态地关注序列的不同部分，从而更好地捕捉序列内部的依赖关系。\n",
    "- **处理长序列**：相比于传统的 Seq2Seq 模型，自注意力机制在处理长序列时表现更好，因为它不需要将整个序列压缩成一个固定大小的向量。\n",
    "\n",
    "### 4.2 局限性\n",
    "\n",
    "- **计算复杂度**：自注意力机制的计算复杂度较高，尤其是在处理长序列时。\n",
    "- **可解释性**：虽然自注意力机制提高了模型的性能，但它也使得模型的可解释性降低，因为注意力权重是动态计算的，难以直观理解。\n",
    "\n",
    "## 5. 应用场景\n",
    "\n",
    "- **机器翻译**：在生成翻译后的句子时，自注意力机制可以帮助模型关注源语言句子中的重要部分。\n",
    "- **文本摘要**：在生成摘要时，自注意力机制可以帮助模型关注原文中的关键信息。\n",
    "- **问答系统**：在生成回答时，自注意力机制可以帮助模型关注问题中的关键部分和相关上下文。\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "自注意力机制是一种在深度学习模型中广泛使用的机制，特别是在处理序列数据时。通过动态地关注序列中的其他部分，自注意力机制显著提高了模型的性能，尤其是在处理长序列和复杂任务时。尽管计算复杂度较高，但自注意力机制在许多 NLP 任务中表现出色，成为现代深度学习模型的核心组件之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "                                   num_hiddens, num_heads, 0.5)\n",
    "attention.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 位置编码（Positional Encoding）\n",
    "\n",
    "位置编码（Positional Encoding）是一种在自然语言处理（NLP）任务中广泛使用的机制，特别是在 Transformer 模型中。由于 Transformer 模型本身不具备处理序列顺序的能力，位置编码被引入以捕捉序列中元素的位置信息。\n",
    "\n",
    "## 1. 背景\n",
    "\n",
    "在传统的序列到序列（Seq2Seq）模型中，如循环神经网络（RNN）、长短期记忆网络（LSTM）和门控循环单元（GRU），序列的顺序信息是通过时间步（Time Step）隐式地捕捉的。然而，Transformer 模型完全基于自注意力机制（Self-Attention），不依赖于时间步，因此需要显式地引入位置信息。\n",
    "\n",
    "## 2. 位置编码的核心思想\n",
    "\n",
    "位置编码的核心思想是为序列中的每个元素添加一个位置向量，该向量表示元素在序列中的位置。通过将位置向量与元素的嵌入向量相加，模型可以同时捕捉元素的语义信息和位置信息。\n",
    "\n",
    "## 3. 工作原理\n",
    "\n",
    "### 3.1 位置编码的计算\n",
    "\n",
    "位置编码通常使用正弦和余弦函数来生成，以确保编码的周期性和平滑性。具体来说，位置编码的计算公式如下：\n",
    "\n",
    "- **公式**：\n",
    "  \\[\n",
    "  PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "  \\]\n",
    "  \\[\n",
    "  PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "  \\]\n",
    "  其中，\\( pos \\) 是位置索引，\\( i \\) 是维度索引，\\( d_{\\text{model}} \\) 是模型的维度。\n",
    "\n",
    "### 3.2 位置编码的添加\n",
    "\n",
    "计算得到位置编码后，将其与输入序列的嵌入向量相加，得到包含位置信息的嵌入向量。\n",
    "\n",
    "- **公式**：\n",
    "  \\[\n",
    "  \\text{embedding\\_with\\_position} = \\text{embedding} + \\text{positional\\_encoding}\n",
    "  \\]\n",
    "\n",
    "## 4. 优点与局限性\n",
    "\n",
    "### 4.1 优点\n",
    "\n",
    "- **捕捉位置信息**：位置编码允许模型捕捉序列中元素的位置信息，从而更好地理解序列的顺序。\n",
    "- **平滑性**：使用正弦和余弦函数生成的位置编码具有平滑性，有助于模型的训练和泛化。\n",
    "\n",
    "### 4.2 局限性\n",
    "\n",
    "- **固定编码**：位置编码是固定的，无法动态调整。对于非常长的序列，固定的位置编码可能无法充分捕捉位置信息。\n",
    "- **计算复杂度**：虽然位置编码的计算复杂度不高，但在处理非常长的序列时，仍然可能成为瓶颈。\n",
    "\n",
    "## 5. 应用场景\n",
    "\n",
    "- **机器翻译**：在生成翻译后的句子时，位置编码可以帮助模型理解源语言句子中的顺序信息。\n",
    "- **文本摘要**：在生成摘要时，位置编码可以帮助模型理解原文中的顺序信息。\n",
    "- **问答系统**：在生成回答时，位置编码可以帮助模型理解问题中的顺序信息和上下文关系。\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "位置编码是一种在 Transformer 模型中广泛使用的机制，用于捕捉序列中元素的位置信息。通过将位置编码与元素的嵌入向量相加，模型可以同时捕捉元素的语义信息和位置信息。尽管位置编码具有一定的局限性，但它在许多 NLP 任务中表现出色，成为现代深度学习模型的核心组件之一。\n",
    "\n",
    "## 简单代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 创建一个足够长的P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
