{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN(全卷积网络)\n",
    "---\n",
    "FCN（Fully Convolutional Network，全卷积网络）是一种深度学习架构，由Jonathan Long、Evan Shelhamer和Trevor Darrell于2015年提出。它是第一个将卷积神经网络（CNN）应用于图像语义分割任务的模型。FCN的主要贡献在于将传统的CNN中的全连接层替换为卷积层，从而使得网络能够接受任意大小的输入图像，并输出与输入图像尺寸相同的分割结果。\n",
    "\n",
    "## 主要特点\n",
    "1. 全卷积结构：\n",
    "\n",
    "    - FCN去除了传统CNN中的全连接层，取而代之的是全卷积层。这意味着网络中的每一层都是卷积层，从而使得网络可以处理任意大小的输入图像。\n",
    "\n",
    "2. 上采样（Upsampling）：\n",
    "\n",
    "    - 由于卷积操作会降低特征图的分辨率，FCN通过上采样操作（如反卷积或双线性插值）来恢复特征图的分辨率，使其与输入图像的尺寸相同。\n",
    "\n",
    "3. 跳跃连接（Skip Connections）：\n",
    "\n",
    "    - FCN引入了跳跃连接，将不同层次的特征图结合起来。具体来说，它将深层特征图（具有较高的语义信息）与浅层特征图（具有较高的空间信息）进行融合，从而提高分割的准确性。\n",
    "\n",
    "4. 端到端训练：\n",
    "\n",
    "    - FCN可以进行端到端的训练，即输入图像经过网络后直接输出分割结果，无需额外的后处理步骤。\n",
    "\n",
    "## 网络结构\n",
    "FCN的网络结构通常基于预训练的CNN（如VGG、ResNet等），并在此基础上进行修改：\n",
    "\n",
    "1. 特征提取：\n",
    "\n",
    "    - 使用预训练的CNN（如VGG-16）进行特征提取，得到一系列不同尺度的特征图。\n",
    "\n",
    "2. 全卷积层：\n",
    "\n",
    "    - 将CNN的最后几层全连接层替换为卷积层，使得网络可以接受任意大小的输入图像。\n",
    "\n",
    "3. 上采样：\n",
    "\n",
    "    - 使用反卷积层对特征图进行上采样，逐步恢复到输入图像的尺寸。\n",
    "\n",
    "4. 跳跃连接：\n",
    "\n",
    "    - 将不同层次的特征图通过跳跃连接进行融合，以提高分割的准确性。\n",
    "\n",
    "![FCN](https://zh-v2.d2l.ai/_images/fcn.svg \"FCN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化转置卷积层\n",
    "在图像处理中，我们有时需要将图像放大，即上采样（upsampling）。 双线性插值（bilinear interpolation） 是常用的上采样方法之一，它也经常用于初始化转置卷积层。\n",
    "\n",
    "为了解释双线性插值，假设给定输入图像，我们想要计算上采样输出图像上的每个像素。\n",
    "\n",
    "1. 将输出图像的坐标(x,y)映射到输入图像的坐标(x',y')上。 例如，根据输入与输出的尺寸之比来映射。 请注意，映射后的x'和y'是实数。\n",
    "\n",
    "2. 在输入图像上找到离坐标(x',y')最近的4个像素。\n",
    "\n",
    "3. 输出图像在坐标(x,y)上的像素依据输入图像上这4个像素及其与(x',y')的相对距离来计算。\n",
    "\n",
    "双线性插值的上采样可以通过转置卷积层实现，内核由以下bilinear_kernel函数构造。 限于篇幅，我们只给出bilinear_kernel函数的实现，不讨论算法的原理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "          torch.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "           (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
