{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "---\n",
    "## 构架概述\n",
    "1. *卷积层（Convolutional Layer）：*\n",
    "\n",
    "    - 卷积核（Filter）：卷积层的核心是卷积核（也称为滤波器），它是一个小的权重矩阵，通过在输入数据上滑动并执行卷积操作来提取特征。\n",
    "\n",
    "    - 特征图（Feature Map）：卷积操作的结果称为特征图，它表示输入数据在特定卷积核下的响应。\n",
    "\n",
    "    - 多通道：输入数据通常是多通道的（例如RGB图像有3个通道），卷积核也会相应地有多个通道，以处理不同通道的信息。\n",
    "\n",
    "2. *激活函数（Activation Function）：*\n",
    "\n",
    "    - 在卷积层之后通常会应用一个非线性激活函数（如ReLU），以引入非线性特性，增强模型的表达能力。\n",
    "\n",
    "3. *池化层（Pooling Layer）：\n",
    "*\n",
    "    - 目的：池化层用于减少特征图的尺寸，从而减少计算量和参数数量，同时保留重要的特征。\n",
    "\n",
    "    - 类型：常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。\n",
    "\n",
    "4. *全连接层（Fully Connected Layer）：*\n",
    "\n",
    "    - 在卷积和池化层之后，通常会连接一个或多个全连接层，用于将高维的特征图转换为一维的特征向量，并最终输出分类结果。\n",
    "\n",
    "5. *归一化层（Normalization Layer）：*\n",
    "\n",
    "    - 如局部响应归一化（Local Response Normalization, LRN），虽然现在较少使用，但在早期CNN中常见。\n",
    "\n",
    "6. *Dropout：*\n",
    "\n",
    "    - 在全连接层中使用Dropout技术，以减少过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 典型结构\n",
    "\n",
    "一个典型的CNN结构通常包括以下几个部分：\n",
    "\n",
    "- *输入层：*接收原始图像数据。\n",
    "\n",
    "- *卷积层：*提取图像的局部特征。\n",
    "\n",
    "- *激活层：*引入非线性。\n",
    "\n",
    "- *池化层：*减少特征图尺寸。\n",
    "\n",
    "- *全连接层：*将特征图转换为分类结果。\n",
    "\n",
    "- *输出层：*使用softmax等函数输出最终分类结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流程图 从LeNet（左）到AlexNet（右）\n",
    "\n",
    "![流程图](https://zh-v2.d2l.ai/_images/alexnet.svg \"LeNet to AlexNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet和LeNet的设计理念非常相似，但也存在显著差异。\n",
    "\n",
    "1. AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。\n",
    "\n",
    "2. AlexNet使用ReLU而不是sigmoid作为其激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简易代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 这里使用一个11*11的更大窗口来捕捉对象。\n",
    "    # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "    # 另外，输出通道的数目远大于LeNet\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "    # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "    nn.Linear(6400, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "    nn.Linear(4096, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们构造一个高度和宽度都为224的单通道数据，来观察每一层输出的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
      "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten output shape:\t torch.Size([1, 6400])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
