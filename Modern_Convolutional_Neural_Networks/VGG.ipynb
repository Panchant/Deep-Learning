{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG\n",
    "---\n",
    "VGG（Visual Geometry Group）网络是由牛津大学的Visual Geometry Group团队在2014年提出的一种深度卷积神经网络架构。它在当年的ImageNet图像分类挑战赛中取得了优异的成绩，并因其简洁的结构和强大的性能而广受欢迎。\n",
    "\n",
    "## 主要特点\n",
    "1. *深度：*VGG网络以其深度著称，最深的版本（VGG19）有19层（包括卷积层和全连接层），而VGG16有16层。这种深度在当时是非常先进的。\n",
    "\n",
    "2. *小卷积核：*VGG网络主要使用3x3的小卷积核，而不是像AlexNet那样使用较大的卷积核（如11x11）。使用多个3x3卷积核的堆叠可以模拟更大感受野的效果，同时减少参数数量。\n",
    "\n",
    "3. *固定结构：*VGG网络的结构非常规整，每层卷积层后面都跟着一个ReLU激活函数，然后是最大池化层。这种固定的结构使得网络的设计和实现非常简单。\n",
    "\n",
    "4. *全连接层：*VGG网络在卷积层之后使用了三个全连接层，最后是一个softmax层用于分类。全连接层的参数数量非常多，这也是VGG网络的一个缺点。\n",
    "\n",
    "## 网络结构\n",
    "\n",
    "VGG网络有多个版本，最常见的是VGG16和VGG19。以下是VGG16的结构：\n",
    "\n",
    "1. *卷积层：*\n",
    "\n",
    "    - 前两个卷积块（Block）各有2个卷积层，每个卷积层使用3x3的卷积核，步幅为1，填充为1。\n",
    "\n",
    "    - 后三个卷积块各有3个卷积层，每个卷积层同样使用3x3的卷积核，步幅为1，填充为1。\n",
    "\n",
    "    - 每个卷积层后面都跟着一个ReLU激活函数。\n",
    "\n",
    "2. 池化层：\n",
    "\n",
    "    - 每个卷积块后面都有一个最大池化层，使用2x2的池化窗口，步幅为2。\n",
    "\n",
    "3. 全连接层：\n",
    "\n",
    "    - 经过5个卷积块后，特征图被展平成一个向量，然后通过三个全连接层。\n",
    "\n",
    "    - 前两个全连接层各有4096个神经元，最后一个全连接层有1000个神经元（对应ImageNet的1000个类别）。\n",
    "\n",
    "    - 每个全连接层后面都跟着一个ReLU激活函数和一个Dropout层（Dropout率为0.5）。\n",
    "\n",
    "4. 输出层：\n",
    "\n",
    "    - 最后一个全连接层后面是一个softmax层，用于输出分类概率。\n",
    "\n",
    "![VGG](https://zh-v2.d2l.ai/_images/vgg.svg \"VGG\")\n",
    "\n",
    "\n",
    "## 优点\n",
    "    简洁性：VGG网络的结构非常简洁，易于理解和实现。\n",
    "\n",
    "    深度：通过增加网络深度，VGG网络能够学习到更复杂的特征。\n",
    "\n",
    "    小卷积核：使用多个3x3卷积核的堆叠可以模拟更大感受野的效果，同时减少参数数量。\n",
    "\n",
    "## 缺点\n",
    "    参数数量多：尽管卷积层的参数数量相对较少，但全连接层的参数数量非常多，导致整个网络的参数量非常大，训练和推理的计算成本较高。\n",
    "\n",
    "    内存消耗大：由于网络深度较大，训练时需要大量的内存来存储中间特征图。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码实现了VGG-11。可以通过在conv_arch上执行for循环来简单实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels,\n",
    "                                kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # 卷积层部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(),\n",
    "        # 全连接层部分\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10))\n",
    "\n",
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将构建一个高度和宽度为224的单通道数据样本，以观察每个层输出的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
