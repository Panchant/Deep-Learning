{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 长短期记忆网络（LSTM）\n",
    "\n",
    "长短期记忆网络（Long Short-Term Memory, LSTM）是一种特殊的循环神经网络（RNN），旨在解决传统RNN在处理长序列数据时遇到的梯度消失和梯度爆炸问题。LSTM通过引入门控机制来控制信息的流动，从而更有效地捕捉序列中的长期依赖关系。\n",
    "\n",
    "## 基本结构\n",
    "\n",
    "LSTM的核心思想是通过三个门控单元（输入门、遗忘门和输出门）来控制隐藏状态的更新。这些门控单元决定了哪些信息应该被保留，哪些信息应该被遗忘，以及哪些信息应该被输出。\n",
    "\n",
    "## 数学表示\n",
    "\n",
    "假设输入序列为 \\( x_1, x_2, \\dots, x_T \\)，隐藏状态序列为 \\( h_1, h_2, \\dots, h_T \\)，细胞状态序列为 \\( c_1, c_2, \\dots, c_T \\)。LSTM的基本更新公式如下：\n",
    "\n",
    "1. **遗忘门（Forget Gate）**：\n",
    "\\[\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "\\]\n",
    "\n",
    "2. **输入门（Input Gate）**：\n",
    "\\[\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "\\]\n",
    "\n",
    "3. **候选细胞状态（Candidate Cell State）**：\n",
    "\\[\n",
    "\\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)\n",
    "\\]\n",
    "\n",
    "4. **更新细胞状态（Update Cell State）**：\n",
    "\\[\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "\\]\n",
    "\n",
    "5. **输出门（Output Gate）**：\n",
    "\\[\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "\\]\n",
    "\n",
    "6. **最终隐藏状态（Final Hidden State）**：\n",
    "\\[\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "- \\( f_t \\) 是遗忘门，控制细胞状态中保留多少前一个时间步的信息。\n",
    "- \\( i_t \\) 是输入门，控制当前时间步的输入信息对细胞状态的贡献。\n",
    "- \\( \\tilde{c}_t \\) 是候选细胞状态，表示在当前时间步可能的细胞状态。\n",
    "- \\( c_t \\) 是最终的细胞状态。\n",
    "- \\( o_t \\) 是输出门，控制细胞状态对隐藏状态的贡献。\n",
    "- \\( h_t \\) 是最终的隐藏状态。\n",
    "- \\( \\sigma \\) 是sigmoid激活函数，用于将输入压缩到0到1之间。\n",
    "- \\( \\tanh \\) 是双曲正切激活函数，用于将输入压缩到-1到1之间。\n",
    "- \\( \\odot \\) 表示逐元素相乘。\n",
    "![LSTM](https://zh-v2.d2l.ai/_images/lstm-3.svg \"LSTM\")\n",
    "## 特点\n",
    "\n",
    "1. **门控机制**：LSTM通过三个门控单元（遗忘门、输入门和输出门）来控制信息的流动，有效地捕捉序列中的长期依赖关系。\n",
    "2. **细胞状态**：LSTM引入细胞状态（Cell State）作为信息的主要载体，通过遗忘门和输入门来控制信息的更新和遗忘。\n",
    "3. **缓解梯度问题**：LSTM的门控机制和细胞状态设计，有效缓解了传统RNN中的梯度消失和梯度爆炸问题。\n",
    "\n",
    "## 应用\n",
    "\n",
    "LSTM广泛应用于各种序列建模任务，特别是在自然语言处理（NLP）领域：\n",
    "- **语言模型**：预测下一个词或字符的概率。\n",
    "- **机器翻译**：将一种语言的句子翻译成另一种语言。\n",
    "- **语音识别**：将语音信号转换为文本。\n",
    "- **时间序列预测**：如股票价格预测、天气预报等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hq, b_q] = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = npx.sigmoid(np.dot(X, W_xi) + np.dot(H, W_hi) + b_i)\n",
    "        F = npx.sigmoid(np.dot(X, W_xf) + np.dot(H, W_hf) + b_f)\n",
    "        O = npx.sigmoid(np.dot(X, W_xo) + np.dot(H, W_ho) + b_o)\n",
    "        C_tilda = np.tanh(np.dot(X, W_xc) + np.dot(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * np.tanh(C)\n",
    "        Y = np.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return np.concatenate(outputs, axis=0), (H, C)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
