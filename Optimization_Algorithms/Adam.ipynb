{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam（Adaptive Moment Estimation）\n",
    "\n",
    "Adam（Adaptive Moment Estimation）是一种在机器学习和深度学习中广泛使用的优化算法，用于最小化损失函数。Adam 结合了动量法（Momentum）和 RMSprop 的优点，通过自适应调整每个参数的学习率，适用于大规模数据和参数。\n",
    "\n",
    "## 1. 背景\n",
    "\n",
    "在传统的梯度下降（Gradient Descent）中，学习率是固定的，这可能导致在不同参数上收敛速度不一致。为了解决这个问题，自适应学习率优化算法被提出，如 Adagrad、Adadelta、RMSprop 等。Adam 结合了动量法和 RMSprop 的优点，通过自适应调整每个参数的学习率，适用于大规模数据和参数。\n",
    "\n",
    "## 2. 核心思想\n",
    "\n",
    "Adam 的核心思想是通过计算梯度的一阶矩估计（即均值）和二阶矩估计（即未中心化的方差），自适应调整每个参数的学习率。具体来说，Adam 在每次迭代中不仅考虑当前梯度，还考虑之前的梯度方向，从而加速收敛速度，并减少震荡。\n",
    "\n",
    "## 3. 工作原理\n",
    "\n",
    "### 3.1 公式\n",
    "\n",
    "Adam 的基本公式如下：\n",
    "\n",
    "\\[\n",
    "m_{t+1} = \\beta_1 m_t + (1 - \\beta_1) \\nabla J(\\theta_t)\n",
    "\\]\n",
    "\\[\n",
    "v_{t+1} = \\beta_2 v_t + (1 - \\beta_2) (\\nabla J(\\theta_t))^2\n",
    "\\]\n",
    "\\[\n",
    "\\hat{m}_{t+1} = \\frac{m_{t+1}}{1 - \\beta_1^{t+1}}\n",
    "\\]\n",
    "\\[\n",
    "\\hat{v}_{t+1} = \\frac{v_{t+1}}{1 - \\beta_2^{t+1}}\n",
    "\\]\n",
    "\\[\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\epsilon}\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "- \\( \\theta \\) 是模型参数。\n",
    "- \\( \\eta \\) 是学习率（Learning Rate），控制每次更新的步长。\n",
    "- \\( \\nabla J(\\theta_t) \\) 是损失函数 \\( J \\) 对参数 \\( \\theta \\) 的梯度。\n",
    "- \\( m \\) 和 \\( v \\) 分别是梯度的一阶矩估计（均值）和二阶矩估计（未中心化的方差）。\n",
    "- \\( \\beta_1 \\) 和 \\( \\beta_2 \\) 是衰减率，控制一阶矩估计和二阶矩估计的衰减速度。\n",
    "- \\( \\epsilon \\) 是平滑项，防止除零错误。\n",
    "\n",
    "### 3.2 步骤\n",
    "\n",
    "1. **初始化参数**：随机初始化模型参数 \\( \\theta \\)，一阶矩估计 \\( m \\) 和二阶矩估计 \\( v \\)。\n",
    "2. **计算梯度**：计算损失函数 \\( J \\) 对参数 \\( \\theta \\) 的梯度 \\( \\nabla J(\\theta) \\)。\n",
    "3. **更新一阶矩估计**：根据当前梯度和之前的一阶矩估计更新一阶矩估计 \\( m \\)。\n",
    "4. **更新二阶矩估计**：根据当前梯度和之前的二阶矩估计更新二阶矩估计 \\( v \\)。\n",
    "5. **修正一阶矩估计**：对一阶矩估计进行偏差修正，得到修正后的一阶矩估计 \\( \\hat{m} \\)。\n",
    "6. **修正二阶矩估计**：对二阶矩估计进行偏差修正，得到修正后的二阶矩估计 \\( \\hat{v} \\)。\n",
    "7. **更新参数**：根据修正后的一阶矩估计和二阶矩估计更新参数 \\( \\theta \\)。\n",
    "8. **重复迭代**：重复步骤 2 到步骤 7，直到损失函数收敛或达到预定的迭代次数。\n",
    "\n",
    "## 4. 优点与局限性\n",
    "\n",
    "### 4.1 优点\n",
    "\n",
    "- **自适应学习率**：Adam 通过自适应调整每个参数的学习率，适用于不同参数。\n",
    "- **结合动量和 RMSprop 的优点**：Adam 结合了动量法和 RMSprop 的优点，收敛速度快。\n",
    "- **适用于大规模数据和参数**：Adam 适用于大规模数据和参数，计算效率高。\n",
    "\n",
    "### 4.2 局限性\n",
    "\n",
    "- **需要调整多个超参数**：Adam 需要调整多个超参数，如学习率、衰减率等。\n",
    "- **对初始学习率敏感**：初始学习率选择不当可能导致收敛速度慢或发散。\n",
    "\n",
    "## 5. 应用场景\n",
    "\n",
    "- **线性回归**：Adam 可以用于最小化线性回归的平方误差损失函数。\n",
    "- **逻辑回归**：Adam 可以用于最小化逻辑回归的交叉熵损失函数。\n",
    "- **神经网络**：Adam 可以用于最小化神经网络的损失函数，训练深度学习模型。\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "Adam 是一种在机器学习和深度学习中广泛使用的优化算法，通过自适应调整每个参数的学习率，结合动量法和 RMSprop 的优点，适用于大规模数据和参数。尽管 Adam 存在一些局限性，如需要调整多个超参数和对初始学习率敏感，但它在许多优化问题中表现出色，成为现代机器学习模型的核心组件之一。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
