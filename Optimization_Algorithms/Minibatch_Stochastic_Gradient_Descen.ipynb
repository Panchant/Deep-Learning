{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小批量随机梯度下降（Mini-Batch Stochastic Gradient Descent, Mini-Batch SGD）\n",
    "\n",
    "小批量随机梯度下降（Mini-Batch Stochastic Gradient Descent, Mini-Batch SGD）是一种在机器学习和深度学习中广泛使用的优化算法，用于最小化损失函数。与传统的梯度下降（Gradient Descent）和随机梯度下降（Stochastic Gradient Descent, SGD）不同，Mini-Batch SGD 在每次迭代中使用一小部分样本（小批量）计算梯度，从而在计算效率和收敛稳定性之间取得平衡。\n",
    "\n",
    "## 1. 背景\n",
    "\n",
    "在传统的梯度下降中，每次迭代需要计算整个训练集的梯度，这在处理大规模数据时计算复杂度较高。随机梯度下降通过在每次迭代中随机选择一个样本计算梯度，从而加速训练过程，但更新过程不稳定。为了解决这些问题，小批量随机梯度下降被提出，通过在每次迭代中使用一小部分样本计算梯度，从而在计算效率和收敛稳定性之间取得平衡。\n",
    "\n",
    "## 2. 核心思想\n",
    "\n",
    "小批量随机梯度下降的核心思想是通过在每次迭代中随机选择一小部分样本（小批量）计算梯度，沿着梯度的反方向更新参数，以逐步减小损失函数的值。由于每次迭代使用多个样本，Mini-Batch SGD 的更新过程比 SGD 更稳定，同时计算效率也比传统的梯度下降高。\n",
    "\n",
    "## 3. 工作原理\n",
    "\n",
    "### 3.1 公式\n",
    "\n",
    "小批量随机梯度下降的基本公式如下：\n",
    "\n",
    "\\[\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla J(\\theta_t; B_t)\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "- \\( \\theta \\) 是模型参数。\n",
    "- \\( \\eta \\) 是学习率（Learning Rate），控制每次更新的步长。\n",
    "- \\( \\nabla J(\\theta_t; B_t) \\) 是损失函数 \\( J \\) 对参数 \\( \\theta \\) 的梯度，基于随机选择的小批量样本 \\( B_t \\)。\n",
    "\n",
    "### 3.2 步骤\n",
    "\n",
    "1. **初始化参数**：随机初始化模型参数 \\( \\theta \\)。\n",
    "2. **随机选择小批量样本**：从训练集中随机选择一小部分样本 \\( B_t \\)。\n",
    "3. **计算梯度**：计算损失函数 \\( J \\) 对参数 \\( \\theta \\) 的梯度 \\( \\nabla J(\\theta; B_t) \\)。\n",
    "4. **更新参数**：沿着梯度的反方向更新参数 \\( \\theta \\)。\n",
    "5. **重复迭代**：重复步骤 2 到步骤 4，直到损失函数收敛或达到预定的迭代次数。\n",
    "\n",
    "## 4. 优点与局限性\n",
    "\n",
    "### 4.1 优点\n",
    "\n",
    "- **计算效率高**：每次迭代使用一小部分样本计算梯度，计算复杂度较低，适用于大规模数据集。\n",
    "- **收敛稳定性高**：由于每次迭代使用多个样本，梯度计算更准确，更新过程更稳定。\n",
    "- **有助于跳出局部最优解**：由于更新过程具有一定的随机性，Mini-Batch SGD 有助于跳出局部最优解。\n",
    "\n",
    "### 4.2 局限性\n",
    "\n",
    "- **需要调整小批量大小**：小批量大小选择不当可能导致收敛速度慢或发散。\n",
    "- **对学习率敏感**：学习率选择不当可能导致收敛速度慢或发散。\n",
    "\n",
    "## 5. 应用场景\n",
    "\n",
    "- **线性回归**：Mini-Batch SGD 可以用于最小化线性回归的平方误差损失函数。\n",
    "- **逻辑回归**：Mini-Batch SGD 可以用于最小化逻辑回归的交叉熵损失函数。\n",
    "- **神经网络**：Mini-Batch SGD 可以用于最小化神经网络的损失函数，训练深度学习模型。\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "小批量随机梯度下降是一种在机器学习和深度学习中广泛使用的优化算法，通过在每次迭代中随机选择一小部分样本计算梯度，从而在计算效率和收敛稳定性之间取得平衡。尽管 Mini-Batch SGD 存在一些局限性，如需要调整小批量大小和对学习率敏感，但它在处理大规模数据时表现出色，成为现代机器学习模型的核心组件之一。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
