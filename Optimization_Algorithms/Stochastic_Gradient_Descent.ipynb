{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降（Stochastic Gradient Descent, SGD）\n",
    "\n",
    "随机梯度下降（Stochastic Gradient Descent, SGD）是一种在机器学习和深度学习中广泛使用的优化算法，用于最小化损失函数。与传统的梯度下降（Gradient Descent）不同，SGD 在每次迭代中随机选择一个样本计算梯度，从而加速训练过程。\n",
    "\n",
    "## 1. 背景\n",
    "\n",
    "在传统的梯度下降中，每次迭代需要计算整个训练集的梯度，这在处理大规模数据时计算复杂度较高。为了解决这个问题，随机梯度下降被提出，通过在每次迭代中随机选择一个样本计算梯度，从而加速训练过程。\n",
    "\n",
    "## 2. 核心思想\n",
    "\n",
    "随机梯度下降的核心思想是通过在每次迭代中随机选择一个样本计算梯度，沿着梯度的反方向更新参数，以逐步减小损失函数的值。由于每次迭代只使用一个样本，SGD 的更新过程具有较大的随机性，有助于跳出局部最优解。\n",
    "\n",
    "## 3. 工作原理\n",
    "\n",
    "### 3.1 公式\n",
    "\n",
    "随机梯度下降的基本公式如下：\n",
    "\n",
    "\\[\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla J(\\theta_t; x_i, y_i)\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "- \\( \\theta \\) 是模型参数。\n",
    "- \\( \\eta \\) 是学习率（Learning Rate），控制每次更新的步长。\n",
    "- \\( \\nabla J(\\theta_t; x_i, y_i) \\) 是损失函数 \\( J \\) 对参数 \\( \\theta \\) 的梯度，基于随机选择的样本 \\( (x_i, y_i) \\)。\n",
    "\n",
    "### 3.2 步骤\n",
    "\n",
    "1. **初始化参数**：随机初始化模型参数 \\( \\theta \\)。\n",
    "2. **随机选择样本**：从训练集中随机选择一个样本 \\( (x_i, y_i) \\)。\n",
    "3. **计算梯度**：计算损失函数 \\( J \\) 对参数 \\( \\theta \\) 的梯度 \\( \\nabla J(\\theta; x_i, y_i) \\)。\n",
    "4. **更新参数**：沿着梯度的反方向更新参数 \\( \\theta \\)。\n",
    "5. **重复迭代**：重复步骤 2 到步骤 4，直到损失函数收敛或达到预定的迭代次数。\n",
    "\n",
    "## 4. 优点与局限性\n",
    "\n",
    "### 4.1 优点\n",
    "\n",
    "- **计算效率高**：每次迭代只使用一个样本计算梯度，计算复杂度低，适用于大规模数据集。\n",
    "- **有助于跳出局部最优解**：由于更新过程具有较大的随机性，SGD 有助于跳出局部最优解。\n",
    "\n",
    "### 4.2 局限性\n",
    "\n",
    "- **更新过程不稳定**：由于每次迭代只使用一个样本，梯度计算可能不准确，导致更新过程不稳定，收敛速度慢。\n",
    "- **对学习率敏感**：学习率选择不当可能导致收敛速度慢或发散。\n",
    "\n",
    "## 5. 应用场景\n",
    "\n",
    "- **线性回归**：SGD 可以用于最小化线性回归的平方误差损失函数。\n",
    "- **逻辑回归**：SGD 可以用于最小化逻辑回归的交叉熵损失函数。\n",
    "- **神经网络**：SGD 可以用于最小化神经网络的损失函数，训练深度学习模型。\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "随机梯度下降是一种在机器学习和深度学习中广泛使用的优化算法，通过在每次迭代中随机选择一个样本计算梯度，从而加速训练过程。尽管 SGD 存在一些局限性，如更新过程不稳定和对学习率敏感，但它在处理大规模数据时表现出色，成为现代机器学习模型的核心组件之一。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
