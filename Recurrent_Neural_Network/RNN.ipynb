{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "---\n",
    "循环神经网络（Recurrent Neural Network, RNN）是一种专门设计用于处理序列数据的神经网络。与传统的前馈神经网络不同，RNN具有内部记忆机制，能够捕捉序列数据中的时间依赖关系。\n",
    "## 基本结构\n",
    "RNN的核心思想是使用循环结构来处理序列数据。在每个时间步，RNN会接收一个输入（如一个词或字符），并结合前一个时间步的隐藏状态来更新当前的隐藏状态。这个隐藏状态可以看作是网络的“记忆”，它包含了之前时间步的信息。\n",
    "\n",
    "## 数学表示\n",
    "\n",
    "\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h).\n",
    "\n",
    "## 特点\n",
    "1. 记忆机制：RNN能够通过隐藏状态保留之前时间步的信息，适用于处理具有时间依赖性的序列数据。\n",
    "\n",
    "2. 序列处理：RNN可以处理任意长度的输入序列，输出序列的长度也可以根据任务需求进行调整。\n",
    "\n",
    "3. 参数共享：RNN在每个时间步使用相同的权重矩阵，这使得模型参数较少，便于训练。\n",
    "\n",
    "## 挑战\n",
    "1. 梯度消失和梯度爆炸：由于RNN的循环结构，在反向传播过程中，梯度可能会随着时间步的增加而消失或爆炸，导致模型难以训练。\n",
    "\n",
    "2. 长距离依赖问题：RNN在处理长序列时，难以捕捉远距离的时间依赖关系。\n",
    "\n",
    "## 应用\n",
    "RNN及其变体在自然语言处理、语音识别、时间序列预测等领域有广泛应用：\n",
    "\n",
    "1. 语言模型：预测下一个词或字符的概率。\n",
    "\n",
    "2. 机器翻译：将一种语言的句子翻译成另一种语言。\n",
    "\n",
    "3. 语音识别：将语音信号转换为文本。\n",
    "\n",
    "4. 时间序列预测：如股票价格预测、天气预报等。\n",
    "\n",
    "## 困惑度（Perplexity）\n",
    "在RNN（循环神经网络）中，困惑度（Perplexity）是评估语言模型的一种方式，用于量化模型生成文本的质量。具体来说，困惑度定义为模型对给定测试集的预测概率的反向度量，计算公式如下：\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>exp</mi>\n",
    "  <mo data-mjx-texclass=\"NONE\">&#x2061;</mo>\n",
    "  <mrow data-mjx-texclass=\"INNER\">\n",
    "    <mo data-mjx-texclass=\"OPEN\">(</mo>\n",
    "    <mo>&#x2212;</mo>\n",
    "    <mfrac>\n",
    "      <mn>1</mn>\n",
    "      <mi>n</mi>\n",
    "    </mfrac>\n",
    "    <munderover>\n",
    "      <mo data-mjx-texclass=\"OP\">&#x2211;</mo>\n",
    "      <mrow data-mjx-texclass=\"ORD\">\n",
    "        <mi>t</mi>\n",
    "        <mo>=</mo>\n",
    "        <mn>1</mn>\n",
    "      </mrow>\n",
    "      <mi>n</mi>\n",
    "    </munderover>\n",
    "    <mi>log</mi>\n",
    "    <mo data-mjx-texclass=\"NONE\">&#x2061;</mo>\n",
    "    <mi>P</mi>\n",
    "    <mo stretchy=\"false\">(</mo>\n",
    "    <msub>\n",
    "      <mi>x</mi>\n",
    "      <mi>t</mi>\n",
    "    </msub>\n",
    "    <mo>&#x2223;</mo>\n",
    "    <msub>\n",
    "      <mi>x</mi>\n",
    "      <mrow data-mjx-texclass=\"ORD\">\n",
    "        <mi>t</mi>\n",
    "        <mo>&#x2212;</mo>\n",
    "        <mn>1</mn>\n",
    "      </mrow>\n",
    "    </msub>\n",
    "    <mo>,</mo>\n",
    "    <mo>&#x2026;</mo>\n",
    "    <mo>,</mo>\n",
    "    <msub>\n",
    "      <mi>x</mi>\n",
    "      <mn>1</mn>\n",
    "    </msub>\n",
    "    <mo stretchy=\"false\">)</mo>\n",
    "    <mo data-mjx-texclass=\"CLOSE\">)</mo>\n",
    "  </mrow>\n",
    "  <mo>.</mo>\n",
    "</math>\n",
    "\n",
    "困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。 我们看看一些案例。\n",
    "\n",
    "在最好的情况下，模型总是完美地估计标签词元的概率为1。 在这种情况下，模型的困惑度为1。\n",
    "\n",
    "在最坏的情况下，模型总是预测标签词元的概率为0。 在这种情况下，困惑度是正无穷大。\n",
    "\n",
    "在基线上，该模型的预测是词表的所有可用词元上的均匀分布。 在这种情况下，困惑度等于词表中唯一词元的数量。 事实上，如果我们在没有任何压缩的情况下存储序列， 这将是我们能做的最好的编码方式。 因此，这种方式提供了一个重要的上限， 而任何实际模型都必须超越这个上限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](https://zh-v2.d2l.ai/_images/rnn-train.svg \"RNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单代码实现\n",
    "\n",
    "### 独热编码\n",
    "独热编码（One-Hot Encoding）是一种常用的数据预处理技术，特别是在处理分类数据时。它将分类变量转换为二进制向量，其中只有一个元素为1，其余元素为0。这种编码方式使得分类变量可以被机器学习算法直接处理。我们每次采样的小批量数据形状是二维张量： （批量大小，时间步数）。 one_hot函数将这样一个小批量数据转换成三维张量， 张量的最后一个维度等于词表大小（len(vocab)）。 我们经常转换输入的维度，以便获得形状为 （时间步数，批量大小，词表大小）的输出。 这将使我们能够更方便地通过最外层的维度， 一步一步地更新小批量数据的隐状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 循环神经网络模型\n",
    "我们首先需要一个init_rnn_state函数在初始化时返回隐状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的rnn函数定义了如何在一个时间步内计算隐状态和输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # X的形状：(批量大小，词表大小)\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们创建一个类来包装这些函数， 并存储从零开始实现的循环神经网络模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModelScratch: #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device,\n",
    "                 get_params, init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "在循环遍历prefix中的开始字符时， 我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。 这被称为预热（warm-up）期， 因为在此期间模型会自我更新（例如，更新隐状态）， 但不会进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度裁剪\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mi mathvariant=\"bold\">g</mi>\n",
    "  </mrow>\n",
    "  <mo stretchy=\"false\">&#x2190;</mo>\n",
    "  <mo data-mjx-texclass=\"OP\" movablelimits=\"true\">min</mo>\n",
    "  <mrow data-mjx-texclass=\"INNER\">\n",
    "    <mo data-mjx-texclass=\"OPEN\">(</mo>\n",
    "    <mn>1</mn>\n",
    "    <mo>,</mo>\n",
    "    <mfrac>\n",
    "      <mi>&#x3B8;</mi>\n",
    "      <mrow>\n",
    "        <mo data-mjx-texclass=\"ORD\" fence=\"false\" stretchy=\"false\">&#x2016;</mo>\n",
    "        <mrow data-mjx-texclass=\"ORD\">\n",
    "          <mi mathvariant=\"bold\">g</mi>\n",
    "        </mrow>\n",
    "        <mo data-mjx-texclass=\"ORD\" fence=\"false\" stretchy=\"false\">&#x2016;</mo>\n",
    "      </mrow>\n",
    "    </mfrac>\n",
    "    <mo data-mjx-texclass=\"CLOSE\">)</mo>\n",
    "  </mrow>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mi mathvariant=\"bold\">g</mi>\n",
    "  </mrow>\n",
    "  <mo>.</mo>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
